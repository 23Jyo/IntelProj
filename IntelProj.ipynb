{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5tdz46h0kD+4uqPGQSIA6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/23Jyo/IntelProj/blob/main/IntelProj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Necessary Packages"
      ],
      "metadata": {
        "id": "tvFYXrhHkn8j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzuyPmy2Kh8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0064d66a-d169-43d3-82ba-e968db15ad6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.4)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime\n",
            "  Downloading onnxruntime-1.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: onnx, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, humanfriendly, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, onnxruntime, nvidia-cusolver-cu12\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 onnx-1.16.1 onnxruntime-1.18.0\n",
            "Collecting openvino-dev\n",
            "  Downloading openvino_dev-2024.2.0-15519-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (0.7.1)\n",
            "Collecting networkx<=3.1.0 (from openvino-dev)\n",
            "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (1.25.2)\n",
            "Collecting openvino-telemetry>=2023.2.1 (from openvino-dev)\n",
            "  Downloading openvino_telemetry-2024.1.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (2.31.0)\n",
            "Collecting openvino==2024.2.0 (from openvino-dev)\n",
            "  Downloading openvino-2024.2.0-15519-cp310-cp310-manylinux2014_x86_64.whl (38.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.7/38.7 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->openvino-dev) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->openvino-dev) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->openvino-dev) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->openvino-dev) (2024.6.2)\n",
            "Installing collected packages: openvino-telemetry, openvino, networkx, openvino-dev\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.3\n",
            "    Uninstalling networkx-3.3:\n",
            "      Successfully uninstalled networkx-3.3\n",
            "Successfully installed networkx-3.1 openvino-2024.2.0 openvino-dev-2024.2.0 openvino-telemetry-2024.1.0\n",
            "Collecting optimum[openvino]\n",
            "  Downloading optimum-1.20.0-py3-none-any.whl (418 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.4/418.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum[openvino]) (15.0.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum[openvino]) (1.12.1)\n",
            "Requirement already satisfied: transformers[sentencepiece]<4.42.0,>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum[openvino]) (4.41.2)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from optimum[openvino]) (2.3.0+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optimum[openvino]) (24.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optimum[openvino]) (1.25.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum[openvino]) (0.23.4)\n",
            "Collecting datasets (from optimum[openvino])\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optimum-intel[openvino]>=1.16.0 (from optimum[openvino])\n",
            "  Downloading optimum_intel-1.18.0-py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.0/224.0 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[openvino]) (3.15.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[openvino]) (2023.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[openvino]) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[openvino]) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[openvino]) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[openvino]) (4.12.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from optimum-intel[openvino]>=1.16.0->optimum[openvino]) (0.1.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from optimum-intel[openvino]>=1.16.0->optimum[openvino]) (67.7.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from optimum-intel[openvino]>=1.16.0->optimum[openvino]) (1.11.4)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from optimum-intel[openvino]>=1.16.0->optimum[openvino]) (1.16.1)\n",
            "Requirement already satisfied: openvino>=2023.3 in /usr/local/lib/python3.10/dist-packages (from optimum-intel[openvino]>=1.16.0->optimum[openvino]) (2024.2.0)\n",
            "Collecting nncf>=2.11.0 (from optimum-intel[openvino]>=1.16.0->optimum[openvino])\n",
            "  Downloading nncf-2.11.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openvino-tokenizers[transformers] (from optimum-intel[openvino]>=1.16.0->optimum[openvino])\n",
            "  Downloading openvino_tokenizers-2024.2.0.0-138-py3-none-manylinux_2_17_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow>=15.0.0 (from datasets->optimum[openvino])\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[openvino]) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->optimum[openvino])\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[openvino]) (2.0.3)\n",
            "Collecting requests (from huggingface-hub>=0.8.0->optimum[openvino])\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets->optimum[openvino])\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->optimum[openvino])\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[openvino]) (3.9.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[openvino]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[openvino]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[openvino]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[openvino]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[openvino]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[openvino]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[openvino]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[openvino]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[openvino]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[openvino]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[openvino]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[openvino]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[openvino]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum[openvino]) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum[openvino]) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum[openvino]) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum[openvino]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum[openvino]) (0.4.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum[openvino]) (3.20.3)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum[openvino]) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum[openvino]) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[openvino]) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[openvino]) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[openvino]) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[openvino]) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[openvino]) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[openvino]) (4.0.3)\n",
            "Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (4.19.2)\n",
            "Collecting jstyleson>=0.0.2 (from nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino])\n",
            "  Downloading jstyleson-0.0.2.tar.gz (2.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: natsort>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (8.4.0)\n",
            "Collecting ninja<1.12,>=1.10.0.post2 (from nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino])\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openvino-telemetry>=2023.2.0 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (2024.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (5.9.5)\n",
            "Requirement already satisfied: pydot>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (1.4.2)\n",
            "Collecting pymoo>=0.6.0.1 (from nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino])\n",
            "  Downloading pymoo-0.6.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=13.5.2 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (13.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (1.2.2)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (0.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum[openvino]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum[openvino]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum[openvino]) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[openvino]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[openvino]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[openvino]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[openvino]) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->optimum[openvino]) (2.1.5)\n",
            "Collecting tiktoken (from openvino-tokenizers[transformers]->optimum-intel[openvino]>=1.16.0->optimum[openvino])\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (0.18.1)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pydot>=1.4.1->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (3.1.2)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (3.7.1)\n",
            "Requirement already satisfied: autograd>=1.4 in /usr/local/lib/python3.10/dist-packages (from pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (1.6.2)\n",
            "Collecting cma==3.2.2 (from pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino])\n",
            "  Downloading cma-3.2.2-py2.py3-none-any.whl (249 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alive-progress (from pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino])\n",
            "  Downloading alive_progress-3.1.5-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Deprecated (from pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum[openvino]) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.5.2->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.5.2->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (2.16.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (3.5.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd>=1.4->pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (0.18.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.5.2->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (9.4.0)\n",
            "Collecting about-time==4.2.1 (from alive-progress->pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino])\n",
            "  Downloading about_time-4.2.1-py3-none-any.whl (13 kB)\n",
            "Collecting grapheme==0.6.0 (from alive-progress->pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino])\n",
            "  Downloading grapheme-0.6.0.tar.gz (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[openvino]>=1.16.0->optimum[openvino]) (1.14.1)\n",
            "Building wheels for collected packages: jstyleson, grapheme\n",
            "  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jstyleson: filename=jstyleson-0.0.2-py3-none-any.whl size=2385 sha256=24edb3da29820252aeacde16b92d61a6d68bca6edc197e23546babf272be8715\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/51/c6/a1e751db88203e11c6d9ffe4683ca3d8c14b1479639bec1006\n",
            "  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grapheme: filename=grapheme-0.6.0-py3-none-any.whl size=210078 sha256=6cd73789f19aae6dac566988cd20918de54de7563e8fc4cbe3cf4403c8a4f005\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/e1/49/37e6bde9886439057450c494a79b0bef8bbe897a54aebfc757\n",
            "Successfully built jstyleson grapheme\n",
            "Installing collected packages: ninja, jstyleson, grapheme, xxhash, requests, pyarrow, dill, Deprecated, cma, about-time, tiktoken, openvino-tokenizers, multiprocess, alive-progress, pymoo, nncf, datasets, optimum, optimum-intel\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Deprecated-1.2.14 about-time-4.2.1 alive-progress-3.1.5 cma-3.2.2 datasets-2.20.0 dill-0.3.8 grapheme-0.6.0 jstyleson-0.0.2 multiprocess-0.70.16 ninja-1.11.1.1 nncf-2.11.0 openvino-tokenizers-2024.2.0.0 optimum-1.20.0 optimum-intel-1.18.0 pyarrow-16.1.0 pymoo-0.6.1.1 requests-2.32.3 tiktoken-0.7.0 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers huggingface_hub onnx onnxruntime\n",
        "!pip install openvino-dev\n",
        "!pip install optimum[openvino]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifying OpenVino Installation"
      ],
      "metadata": {
        "id": "zwKeo6we2H7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify OpenVINO installation\n",
        "from openvino.runtime import Core\n",
        "core = Core()\n",
        "versions = core.get_versions(\"CPU\")\n",
        "print(\"OpenVINO versions:\", versions)\n",
        "\n",
        "# Set up environment variables for Model Optimizer\n",
        "import os\n",
        "mo_path = '/usr/local/bin/mo'  # Update this path if necessary\n",
        "mo_dir = os.path.dirname(mo_path)\n",
        "\n",
        "os.environ['MO_ROOT'] = mo_dir\n",
        "os.environ['PYTHONPATH'] = f\"{mo_dir}:{os.environ.get('PYTHONPATH', '')}\"\n",
        "os.environ['PATH'] = f\"{mo_dir}:{os.environ.get('PATH', '')}\"\n",
        "\n",
        "# Verify that the Model Optimizer script is accessible\n",
        "if os.path.isfile(mo_path):\n",
        "    print(f\"Model Optimizer script found at: {mo_path}\")\n",
        "else:\n",
        "    print(f\"Model Optimizer script not found at: {mo_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KCCwHrot8SG",
        "outputId": "3904e59a-319b-4831-da69-46a225643c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenVINO versions: {'CPU': <Version: 2024.2.0-15519-5c0f38f83f6-releases/2024/2 openvino_intel_cpu_plugin>}\n",
            "Model Optimizer script found at: /usr/local/bin/mo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Login to Hugging face"
      ],
      "metadata": {
        "id": "XOmJtbBxl2kW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Replace 'your_access_token' with your Hugging Face token\n",
        "login(token='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')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJIsq9blLK4D",
        "outputId": "5040c6fb-93d5-4af4-90b1-ce40e5990f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Model and Tokenizer"
      ],
      "metadata": {
        "id": "FHH02pcFnm7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")\n",
        "\n",
        "# Example input for tracing\n",
        "example_input = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").input_ids\n",
        "\n"
      ],
      "metadata": {
        "id": "5qooK8IRflZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export the model to ONNX"
      ],
      "metadata": {
        "id": "Y39Lm3Z8rqaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.onnx\n",
        "\n",
        "# Export the model to ONNX\n",
        "try:\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        example_input,\n",
        "        \"tiny_llama_1.1B.onnx\",\n",
        "        input_names=[\"input_ids\"],\n",
        "        output_names=[\"output\"],\n",
        "        dynamic_axes={\"input_ids\": {0: \"batch_size\", 1: \"sequence\"}},\n",
        "        opset_version=14\n",
        "    )\n",
        "    print(\"Model exported to ONNX successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error exporting model to ONNX: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "MIoMEQjWLUNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bace476f-e1b9-4a20-c0e4-55c6ea5fd94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported to ONNX successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert ONNX to OpenVino"
      ],
      "metadata": {
        "id": "d3rADUUsscB6"
      }
    },
    {
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Ensure the output directory exists\n",
        "output_dir = \"openvino_model\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Path to the ONNX model\n",
        "onnx_model_path = \"tiny_llama_1.1B.onnx\"\n",
        "\n",
        "# Command to run the Model Optimizer\n",
        "mo_command = f\"mo --input_model {onnx_model_path} --output_dir {output_dir}\"\n",
        "\n",
        "# Run the command and capture the output\n",
        "process = subprocess.run(mo_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "# Print the standard output and standard error\n",
        "print(\"Standard Output:\\n\", process.stdout)\n",
        "print(\"Standard Error:\\n\", process.stderr)\n",
        "\n",
        "# Check the return code to see if the command was successful\n",
        "if process.returncode != 0:\n",
        "    print(\"Model conversion failed.\")\n",
        "else:\n",
        "    print(\"Model conversion completed successfully.\")\n",
        "\n",
        "# Paths to the expected model files\n",
        "xml_file = \"tiny_llama_1.1B.xml\"\n",
        "bin_file = \"tiny_llama_1.1B.bin\"\n",
        "xml_path = os.path.join(output_dir, xml_file)\n",
        "bin_path = os.path.join(output_dir, bin_file)\n",
        "\n",
        "# Check if both files exist\n",
        "if os.path.exists(xml_path) and os.path.exists(bin_path):\n",
        "    print(f\"Model conversion successful. Files found:\\n{xml_file}\\n{bin_file}\")\n",
        "else:\n",
        "    print(\"Model conversion failed or files are missing.\")\n",
        "    if not os.path.exists(xml_path):\n",
        "        print(f\"Missing file: {xml_file}\")\n",
        "    if not os.path.exists(bin_path):\n",
        "        print(f\"Missing file: {bin_file}\")\n",
        "\n",
        "# Print the contents of the output directory\n",
        "print(\"\\nContents of the output directory:\")\n",
        "print(os.listdir(output_dir))\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAcq-WE9tODN",
        "outputId": "4607ec31-78d6-4352-b3d7-9cd20c2408f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard Output:\n",
            " [ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
            "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
            "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
            "[ INFO ] Generated IR will be compressed to FP16. If you get lower accuracy, please consider disabling compression explicitly by adding argument --compress_to_fp16=False.\n",
            "Find more information about compression to FP16 at https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_FP16_Compression.html\n",
            "[ SUCCESS ] Generated IR version 11 model.\n",
            "[ SUCCESS ] XML file: /content/openvino_model/tiny_llama_1.1B.xml\n",
            "[ SUCCESS ] BIN file: /content/openvino_model/tiny_llama_1.1B.bin\n",
            "\n",
            "Standard Error:\n",
            " \n",
            "Model conversion completed successfully.\n",
            "Model conversion successful. Files found:\n",
            "tiny_llama_1.1B.xml\n",
            "tiny_llama_1.1B.bin\n",
            "\n",
            "Contents of the output directory:\n",
            "['tiny_llama_1.1B.bin', 'tiny_llama_1.1B.xml']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the optimized model and perform Inference"
      ],
      "metadata": {
        "id": "7oIIdmsvVeDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openvino.runtime import Core\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# Initialize OpenVINO Core\n",
        "core = Core()\n",
        "\n",
        "# Load the optimized model\n",
        "model_path = \"openvino_model/tiny_llama_1.1B.xml\"\n",
        "compiled_model = core.compile_model(model_path, device_name=\"CPU\")\n",
        "\n",
        "# Prepare the input\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").input_ids.numpy()\n",
        "\n",
        "# Create an inference request\n",
        "infer_request = compiled_model.create_infer_request()\n",
        "\n",
        "# Perform inference\n",
        "infer_request.infer({compiled_model.input(0): inputs})\n",
        "\n",
        "# Get the output (logits)\n",
        "output = infer_request.get_output_tensor(0).data\n",
        "\n",
        "# Check the data type and range of the output tensor\n",
        "print(f\"Output dtype: {output.dtype}\")\n",
        "print(f\"Output range: min={output.min()}, max={output.max()}\")\n",
        "\n",
        "# Define softmax function\n",
        "def softmax(x, temperature=1.0):\n",
        "    x = x / temperature\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
        "\n",
        "# Apply temperature scaling\n",
        "temperature = 0.7\n",
        "probabilities = softmax(output, temperature=temperature)\n",
        "\n",
        "# Check the probabilities\n",
        "print(f\"Probabilities range: min={probabilities.min()}, max={probabilities.max()}\")\n",
        "\n",
        "# Sample token IDs from the probability distribution\n",
        "token_ids = [\n",
        "    np.random.choice(len(prob), p=prob)\n",
        "    for prob in probabilities.reshape(-1, probabilities.shape[-1])\n",
        "]\n",
        "\n",
        "# Ensure the output is correctly shaped\n",
        "token_ids = np.array(token_ids).reshape(probabilities.shape[:-1])\n",
        "\n",
        "# Decode the output token ids to text\n",
        "generated_text = tokenizer.decode(token_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Text:\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zo2WjB7Vdh1",
        "outputId": "6e71b8f7-9d5c-4394-eb25-db51801f7804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output dtype: float32\n",
            "Output range: min=-19.764528274536133, max=17.37258529663086\n",
            "Probabilities range: min=8.18269227792932e-20, max=0.9987313151359558\n",
            "Generated Text: agne World my are you?\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
# Running GenAI on Intel AI Laptops and Simple LLM Inference on CPU

## Problem Statement

This project is designed to introduce beginners to the exciting field of Generative Artificial Intelligence (GenAI) through a series of hands-on exercises. Participants will learn the basics of GenAI, perform simple Large Language Model (LLM) inference on a CPU, and explore the process of fine-tuning an LLM model to create a custom Chatbot.

### Category
- Artificial Intelligence
- Machine Learning
- Large Language Models (LLM)
- Natural Language Processing (NLP)

## Description

This problem statement is designed to introduce beginners to the exciting field of Generative Artificial Intelligence (GenAI) through a series of hands-on exercises. Participants will learn the basics of GenAI, perform simple Large Language Model (LLM) inference on a CPU, and explore the process of fine-tuning an LLM model to create a custom Chatbot.

## Major Challenges

1. **Pre-trained language models can have large file sizes**, which may require significant storage space and memory to load and run.
2. **Learn LLM inference on CPU.**
3. **Understanding the concept of fine-tuning** and its importance in customizing LLMs.
4. **Create a Custom Chatbot with Fine-tuned Pre-trained Large Language Models (LLMs)** using Intel AI Tools.

## Setup and Installation

### Prerequisites

- Python 3.7 or higher
- Intel® OpenVINO™ toolkit
- Hugging Face transformers library
- Other dependencies listed in `requirements.txt`

### Installation

1. Clone the repository:
    ```sh
    git clone <repository-url>
    cd <repository-directory>
    ```

2. Create a virtual environment:
    ```sh
    python -m venv venv
    venv\Scripts\activate  # to activate the environment
    ```

3. Install dependencies:
    ```sh
    pip install -r requirements.txt
    ```

4. Install Intel® OpenVINO™ toolkit:
    Follow the instructions at [Intel OpenVINO Installation Guide](https://docs.openvino.ai/latest/openvino_docs_install_guides_installing_openvino.html)

## Usage

## Libraries used 
-Transformers: For using pre-trained models and performing inference.

-OpenVINO: For optimizing and accelerating the model on Intel hardware.

-Optimum Intel: For integrating Hugging Face models with OpenVINO.

-ONNX: For exporting models to the ONNX format.

-Numpy: For numerical operations.

-Torch: PyTorch for deep learning models (if used).

-Streamlit: is a Python library that allows you to create interactive, web-based applications for data science and machine learning projects.

## Team Members

-[Jyothsna Sara Abey](https://github.com/23Jyo)

-[Aiswarya Rahul](https://github.com/aiswaryarahull)

-[Cinta Susan Thomas](https://github.com/Cinta-Susan-Thomas)

-[Jacksilin P Titus](https://github.com/jacksilin)

-[Tebin Philip George](https://github.com/tebingeorge)
